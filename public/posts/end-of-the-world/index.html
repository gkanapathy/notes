<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
   · Notes
</title>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Gerald Kanapathy">
<meta name="description" content="Overview Link to heading The first three or four episodes are setting up for what existential risk is, why it matters, and what it looks like. The remaining ones cover specific types of risk.
Overall, the series covers the conventional wisdom on this topic well. It doesn&rsquo;t challenge, dig deep, or break new ground, but it&rsquo;s fine.
Background Link to heading Fermi Paradox &amp; Drake Equation Link to heading This sets up the series, and justifies why there&rsquo;s actual evidence that we are at very high risk of going extinct, i.">
<meta name="keywords" content="">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Overview Link to heading The first three or four episodes are setting up for what existential risk is, why it matters, and what it looks like. The remaining ones cover specific types of risk.
Overall, the series covers the conventional wisdom on this topic well. It doesn&rsquo;t challenge, dig deep, or break new ground, but it&rsquo;s fine.
Background Link to heading Fermi Paradox &amp; Drake Equation Link to heading This sets up the series, and justifies why there&rsquo;s actual evidence that we are at very high risk of going extinct, i."/>

<meta property="og:title" content="" />
<meta property="og:description" content="Overview Link to heading The first three or four episodes are setting up for what existential risk is, why it matters, and what it looks like. The remaining ones cover specific types of risk.
Overall, the series covers the conventional wisdom on this topic well. It doesn&rsquo;t challenge, dig deep, or break new ground, but it&rsquo;s fine.
Background Link to heading Fermi Paradox &amp; Drake Equation Link to heading This sets up the series, and justifies why there&rsquo;s actual evidence that we are at very high risk of going extinct, i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/end-of-the-world/" /><meta property="article:section" content="posts" />

<meta property="article:modified_time" content="2022-11-05T17:50:49-07:00" />





<link rel="canonical" href="/posts/end-of-the-world/">


<link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.c4d7e93a158eda5a65b3df343745d2092a0a1e2170feeec909b8a89443903c6a.css" integrity="sha256-xNfpOhWO2lpls980N0XSCSoKHiFw/u7JCbiolEOQPGo=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.78b5fe3864945faf5207fb8fe3ab2320d49c3365def0e88ac1df0ddadc54a03c.css" integrity="sha256-eLX&#43;OGSUX69SB/uP46sjINScM2Xe8OiKwd8N2txUoDw=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">




<meta name="generator" content="Hugo 0.105.0">





  </head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Notes
    </a>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="/posts/end-of-the-world/">
              
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2022-11-05T17:50:49-07:00">
                November 5, 2022
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              4-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div>
        
        <h1 id="overview">
  Overview
  <a class="heading-link" href="#overview">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>The first three or four episodes are setting up for what existential risk is,
why it matters, and what it looks like. The remaining ones cover specific types
of risk.</p>
<p>Overall, the series covers the conventional wisdom on this topic well. It doesn&rsquo;t
challenge, dig deep, or break new ground, but it&rsquo;s fine.</p>
<h1 id="background">
  Background
  <a class="heading-link" href="#background">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<h2 id="fermi-paradox--drake-equation">
  Fermi Paradox &amp; Drake Equation
  <a class="heading-link" href="#fermi-paradox--drake-equation">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>This sets up the series, and justifies why there&rsquo;s actual evidence that we are
at very high risk of going extinct, i.e. why we think there&rsquo;s a Great Filter in
our future. It&rsquo;s pretty good but I think it should be noted there are a couple
of other resolutions to the Fermi Paradox besides the existence of a Great
Filter, i.e., there are good reasons that intelligent life is rare.</p>
<p>Compelling ones to me are:</p>
<ul>
<li>
<p><a href="https://arxiv.org/abs/1806.02404"><em>Dissolving the Fermi Paradox</em></a>
<a href="https://arxiv.org/abs/1806.0240">https://arxiv.org/abs/1806.0240</a> which is basically an argument that the
statistical assumptions in the Drake Equation give wrong conclusions, i.e.,
basically it&rsquo;s inferring things from the mean, while outliers in the
parameters should actually be given much less weight.</p>
</li>
<li>
<p><a href="https://grabbyaliens.com/">Grabby Aliens</a> <a href="https://grabbyaliens.com/">https://grabbyaliens.com/</a> which is
by Robin Hanson, who wrote the original Great Filter paper.</p>
</li>
<li>
<p>There&rsquo;s also an episode of this podcast dedicated to going over these
<a href="http://rationallyspeakingpodcast.org/203-where-is-everybody-solutions-to-the-fermi-paradox-stephen-webb/">http://rationallyspeakingpodcast.org/203-where-is-everybody-solutions-to-the-fermi-paradox-stephen-webb/</a>
which is a more complete overview of the topic. One that sticks out to me is
that there isn&rsquo;t one Great Filter, but the filter is a series of smaller
barriers.</p>
</li>
</ul>
<h2 id="great-filter">
  Great Filter
  <a class="heading-link" href="#great-filter">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>What I said. But also, they underplay the possibility that the Great Filter may
be in the past. More importantly they underplay that the Great Filter isn&rsquo;t a
single step, but a series of smaller barriers that all have to be overcome,
i.e., a species could pass 9/10 steps, <em>not</em> go extinct by never get to space
colonization or whatever, and that would resolve the Fermi Paradox. (e.g., the
dinosaurs could have had an industrial civilation and never made it to space, or
even made it to space a little beyond us, and if they didn&rsquo;t get unlucky could
have just stalled out there.)</p>
<h2 id="x-risks">
  X Risks
  <a class="heading-link" href="#x-risks">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Yeah this is mostly definitions, and yes, we want to be especially wary of things that end the species (or end all life)</p>
<h2 id="natural-risks">
  Natural Risks
  <a class="heading-link" href="#natural-risks">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Not a lot we can do directly about these, other than manage risk and mitigate.
Mostly it comes down to eventually being able to spread the civilization so that
when the sun blows up or there&rsquo;s a gamma ray burst or whatever, we can at least
build back. I buy that.</p>
<h2 id="artificial-intelligence">
  Artificial Intelligence
  <a class="heading-link" href="#artificial-intelligence">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>As I said, fine overview of the conventional conversation about this topic. The
problem I have with the AGI fear is that it&rsquo;s entirely built around the image of
the paperclip maximizer as some kind of genie, like any of our technologies,
like fire or nuclear bombs or the brooms from the Sorcerer&rsquo;s Apprentice.</p>
<p>It just
seems far-fetched to me that we posit some AI that:</p>
<ul>
<li>Can trick us, understand human society well enough to manipulate our
institutions, evade all our safeguards; and at the same time,</li>
<li>Is stuck in the world&rsquo;s stupidest while loop.</li>
</ul>
<p>It can understand human motivations, end war, get everyone to agree to be
vaccinated, etc etc, but at the same time if someone says &ldquo;make everyone happy&rdquo;
it might think we meant to just put everyone onto a heroin drip for the rest of
their lives? I agree that we shouldn&rsquo;t anthromorphize AGI motivations, but in
this case, all we&rsquo;re doing is ascribing two separate and inconsistent human
behaviors and saying &ldquo;see, isn&rsquo;t that scary?&rdquo; Is it a stupid box that can&rsquo;t
break free of it&rsquo;s programming, or is it some dangerous demon that is breaking
free of it&rsquo;s programming?</p>
<p>It&rsquo;s also not obvious to me that an AGI actually <em>could</em> engineer and build
another AGI that was smarter than it, which would the recursively keep doing the
same. (Also, what stops the recursion?)</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2022
     Gerald Kanapathy 
    ·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js" integrity="sha256-I2BJOV3DaC&#43;ycZZAhylY4S8fJAZ7sJwyeyM&#43;YpDH7aw="></script>
  

  

  

  

  

  

  

  

  

  
</body>

</html>
